{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db9abbdf-8b62-4d81-b408-96ab2c029f9b",
   "metadata": {},
   "source": [
    "# a simple and naive implementation for minnn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a49986d8-396f-462a-995f-34329416d977",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../minnn.py\n",
    "#\n",
    "\n",
    "\n",
    "from typing import List, Tuple, Sequence, Union, Any, Dict\n",
    "import math\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0498e79d-7c93-4506-9b6b-7fe3bbcff767",
   "metadata": {},
   "source": [
    "# which *py to use??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d24b488-7b32-4c59-8bc0-c96e6c9c4d0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use numpy!\n"
     ]
    }
   ],
   "source": [
    "# --\n",
    "\n",
    "_WHICH_XP = os.environ.get(\"WHICH_XP\", \"np\")\n",
    "if _WHICH_XP.lower() in [\"cupy\", \"cp\"]:\n",
    "    print(\"Use cupy!\")\n",
    "    import cupy as xp\n",
    "    def asnumpy(x):\n",
    "        return xp.asnumpy(x)\n",
    "else:\n",
    "    print(\"Use numpy!\")\n",
    "    import numpy as xp\n",
    "    def asnumpy(x):\n",
    "        return np.asarray(x)\n",
    "\n",
    "# random seed\n",
    "xp.random.seed(12345)\n",
    "\n",
    "def set_random_seed(seed: int):  # allow reset!\n",
    "    xp.random.seed(seed)\n",
    "# --\n",
    "\n",
    "# --\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2bcd70ab-a355-4572-8bb7-75a931f35793",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../test_minnn.py\n",
    "#\n",
    "\n",
    "# a script to test the minnn.py\n",
    "# -- only with simple input/output examples\n",
    "# -- usage is simply: python test_minnn.py <your-minnn.py>\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(123)\n",
    "\n",
    "# --\n",
    "# helpers\n",
    "\n",
    "def search_file(dir: str, filename: str):\n",
    "    rets = []\n",
    "    for dirpath, _, filenames in os.walk(dir):\n",
    "        if filename in filenames:\n",
    "            rets.append(os.path.join(dirpath, filename))\n",
    "    assert len(rets) == 1\n",
    "    return rets[0]\n",
    "\n",
    "def load_module(f: str, m: str):\n",
    "    import importlib.util\n",
    "    spec = importlib.util.spec_from_file_location(m, f)\n",
    "    ret = importlib.util.module_from_spec(spec)\n",
    "    spec.loader.exec_module(ret)\n",
    "    return ret\n",
    "\n",
    "def is_allclose(x, y):\n",
    "    return np.allclose(x, y, rtol=1.e-3, atol=1.e-5)\n",
    "    # return np.abs(x-y).sum().item() <= 1e-5\n",
    "\n",
    "# --\n",
    "# testings!\n",
    "def test_accumulate_grad(test_mn):\n",
    "    g0 = np.asarray([1., 2.])\n",
    "    g1 = np.asarray([3., 4.])\n",
    "    t = test_mn.astensor([0., 0.])\n",
    "    t.accumulate_grad(g0)\n",
    "    t.accumulate_grad(g1)\n",
    "    assert is_allclose(t.get_dense_grad(), np.asarray([4., 6.]))\n",
    "    # --\n",
    "\n",
    "def test_accumulate_grad_sparse(test_mn):\n",
    "    g0 = np.asarray([1., 2.])\n",
    "    g1 = np.asarray([3., 4.])\n",
    "    t = test_mn.astensor([[0., 0.], [0., 0.], [0., 0.]])\n",
    "    t.accumulate_grad_sparse([(0, g0), (2, g1), (2, g0)])\n",
    "    assert is_allclose(t.get_dense_grad(), np.asarray([g0, [0.,0.], g0+g1]))\n",
    "    # --\n",
    "\n",
    "def test_xavier_uniform(test_mn):\n",
    "    shape = [100, 500]\n",
    "    y = test_mn.Initializer.xavier_uniform(shape)\n",
    "    assert list(y.shape) == shape\n",
    "    assert abs(y.mean()) <= 1e-2\n",
    "    assert abs(y.max()-0.1) <= 1e-2\n",
    "    assert abs(y.min()+0.1) <= 1e-2\n",
    "    # --\n",
    "\n",
    "def test_momentum_update(test_mn):\n",
    "    m1 = test_mn.Model()\n",
    "    p1 = m1.add_parameters([2], 'constant', val=0.)\n",
    "    p2 = m1.add_parameters([2,2], 'constant', val=0.)\n",
    "    # --\n",
    "    t1 = test_mn.MomentumTrainer(m1, lrate=0.1, mrate=0.9)\n",
    "    for _ in range(5):\n",
    "        g = np.array([1.,1.])\n",
    "        p1.accumulate_grad(g)\n",
    "        t1.update()\n",
    "    assert is_allclose(p1.data, np.array([-0.131441, -0.131441])), \\\n",
    "        \"This is the values using our implementation, there can be other versions for momentum_update, you can choose to use others!\"\n",
    "    for i in range(6):\n",
    "        g = np.array([1.,1.])\n",
    "        p2.accumulate_grad_sparse([((i%2), g.copy())])\n",
    "        t1.update()\n",
    "    assert is_allclose(p2.data, np.array([[-0.1002459, -0.1002459], [-0.078051, -0.078051]])), \\\n",
    "        \"This is the values using our implementation, there can be other versions for momentum_update, you can choose to use others!\"\n",
    "    # --\n",
    "\n",
    "def test_lookup(test_mn):\n",
    "    t = test_mn.astensor([[1., 2.], [3., 4.], [5., 6.]])\n",
    "    v = test_mn.lookup(t, [1,0,1])\n",
    "    assert is_allclose(v.data, np.asarray([[3., 4.], [1., 2.], [3., 4.]]))\n",
    "    v.accumulate_grad(np.asarray([[1., 1.], [1., 1.], [2., 3.]]))\n",
    "    v.op.backward()\n",
    "    assert is_allclose(t.get_dense_grad(), np.asarray([[1., 1.], [3., 4.], [0., 0.]]))\n",
    "\n",
    "def test_dot(test_mn):\n",
    "    w, h = test_mn.astensor([[0., 1.], [2., 3.]]), test_mn.astensor([1., 2.])\n",
    "    v = test_mn.dot(w, h)\n",
    "    assert is_allclose(v.data, np.asarray([2., 8.]))\n",
    "    v.accumulate_grad(np.asarray([1., 1.]))\n",
    "    v.op.backward()\n",
    "    assert is_allclose(w.get_dense_grad(), np.asarray([[1.,2.],[1.,2.]]))\n",
    "    assert is_allclose(h.get_dense_grad(), np.asarray([2.,4.]))\n",
    "\n",
    "def test_tanh(test_mn):\n",
    "    x = test_mn.astensor([0., 1., 2., 3.])\n",
    "    v = test_mn.tanh(x)\n",
    "    assert is_allclose(v.data, np.asarray([0., 0.76159416, 0.96402758, 0.99505475]))\n",
    "    v.accumulate_grad(np.asarray([1.,2.,3.,4.]))\n",
    "    v.op.backward()\n",
    "    assert is_allclose(x.get_dense_grad(), np.asarray([1., 0.83994868, 0.21195247, 0.03946415]))\n",
    "    # --\n",
    "\n",
    "# --\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6fb4a3b-cad0-4fb2-9da7-623daf784af3",
   "metadata": {},
   "source": [
    "# Components in computation graph\n",
    "## Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08cbdb64-670e-4d6d-9fc2-6b0d69ee51d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tensor:\n",
    "    def __init__(self, data: xp.ndarray):\n",
    "        self.data: xp.ndarray = data\n",
    "        self.grad: Union[xp.ndarray, Dict[int, xp.ndarray]] = None\n",
    "        self.op: Op = None\n",
    "        \n",
    "    @property\n",
    "    def shape(self):\n",
    "        return self.data.shape\n",
    "    \n",
    "    def __repr__(self):\n",
    "         return f\"T{self.shape}: {self.data}\"\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e667a5af-0227-42d1-9c6b-941601f5ba19",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Tensor:\n",
    "    def __init__(self, data: xp.ndarray):\n",
    "        self.data: xp.ndarray = data\n",
    "        self.grad: Union[Dict[int, xp.ndarray], xp.ndarray] = None  # should be the same size as data\n",
    "        self.op: Op = None  # generated from which operation?\n",
    "\n",
    "    @property\n",
    "    def shape(self):\n",
    "        return self.data.shape\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"T{self.shape}: {self.data}\"\n",
    "\n",
    "    # accumulate grad\n",
    "    def accumulate_grad(self, g: xp.ndarray) -> None:\n",
    "        if isinstance(self.grad, xp.ndarray):\n",
    "            self.grad += g\n",
    "        else:\n",
    "            self.grad = g\n",
    "        return\n",
    "\n",
    "    # accumulate grad sparsely; note: only for D2 lookup matrix!\n",
    "    def accumulate_grad_sparse(self, gs: List[Tuple[int, xp.ndarray]]) -> None:\n",
    "        if self.grad is None: self.grad = {i:0 for i in range(self.shape[0])}\n",
    "        for ind, ar in gs:\n",
    "            self.grad[ind] =  self.grad[ind] + ar\n",
    "\n",
    "        return\n",
    "    # get dense grad\n",
    "    def get_dense_grad(self):\n",
    "        ret = xp.zeros_like(self.data)\n",
    "        if self.grad is not None:\n",
    "            if isinstance(self.grad, dict):\n",
    "                for widx, arr in self.grad.items():\n",
    "                    ret[widx] += arr\n",
    "            else:\n",
    "                ret = self.grad\n",
    "        return ret\n",
    "\n",
    "    # add or sub\n",
    "    def __add__(self, other: 'Tensor'):\n",
    "        return OpAdd().full_forward(self, other)\n",
    "\n",
    "    def __sub__(self, other: 'Tensor'):\n",
    "        return OpAdd().full_forward(self, other, alpha_b=-1.)\n",
    "\n",
    "    def __mul__(self, other: Union[int, float]):\n",
    "        assert isinstance(other, (int, float)), \"currently only support scalar __mul__\"\n",
    "        return OpAdd().full_forward(self, b=None, alpha_a=float(other))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9ae2b3-4bf7-4200-b3ee-4e73659e0cc9",
   "metadata": {},
   "source": [
    "## Parameter: special tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "96e8c54c-5e21-48de-8e66-7e4add90b398",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter: special tensor\n",
    "class Parameter(Tensor):\n",
    "    def __init__(self, data: xp.ndarray):\n",
    "        super().__init__(data)\n",
    "\n",
    "    @classmethod\n",
    "    def from_tensor(cls, tensor: Tensor):\n",
    "        return Parameter(tensor.data)  # currently simply steal its data\n",
    "\n",
    "# shortcut for create tensor\n",
    "def astensor(t):\n",
    "    return t if isinstance(t, Tensor) else Tensor(xp.asarray(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8959ea50-852c-4cc2-b42c-8be5f13addef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_accumulate_grad():\n",
    "    g0 = np.asarray([1., 2.])\n",
    "    g1 = np.asarray([3., 4.])\n",
    "    t = astensor([0., 0.])\n",
    "    t.accumulate_grad(g0)\n",
    "    t.accumulate_grad(g1)\n",
    "    assert is_allclose(t.get_dense_grad(), np.asarray([4., 6.]))\n",
    "    # --\n",
    "test_accumulate_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e758e477-3492-4cfb-9efb-e83c8e4f704d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_accumulate_grad_sparse():\n",
    "    g0 = np.asarray([1., 2.])\n",
    "    g1 = np.asarray([3., 4.])\n",
    "    t = astensor([[0., 0.], [0., 0.], [0., 0.]])\n",
    "    t.accumulate_grad_sparse([(0, g0), (2, g1), (2, g0)])\n",
    "    assert is_allclose(t.get_dense_grad(), np.asarray([g0, [0.,0.], g0+g1]))\n",
    "    # --\n",
    "test_accumulate_grad_sparse()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43fb838-1d6e-4e20-8754-058134646472",
   "metadata": {},
   "source": [
    "## Operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "258d7268-01fa-4cba-8048-bdf35fd56409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Operation\n",
    "class Op:\n",
    "    def __init__(self):\n",
    "        self.ctx: Dict[str, Union[Tensor, Any]] = {}  # store intermediate tensors or other values\n",
    "        self.idx: int = None  # idx in the cg\n",
    "        ComputationGraph.get_cg().reg_op(self)  # register into the graph\n",
    "\n",
    "    # store intermediate results for usage in backward\n",
    "    def store_ctx(self, ctx: Dict = None, **kwargs):\n",
    "        if ctx is not None:\n",
    "            self.ctx.update(ctx)\n",
    "        self.ctx.update(kwargs)\n",
    "\n",
    "    # get stored ctx values\n",
    "    def get_ctx(self, *names: str):\n",
    "        return [self.ctx.get(n) for n in names]\n",
    "\n",
    "    # full forward, forwarding plus set output op\n",
    "    def full_forward(self, *args, **kwargs):\n",
    "        rets = self.forward(*args, **kwargs)\n",
    "        # -- store op for outputs\n",
    "        outputs = []\n",
    "        if isinstance(rets, Tensor):\n",
    "            outputs.append(rets)  # single return\n",
    "        elif isinstance(rets, (list, tuple)):  # note: currently only support list or tuple!!\n",
    "            outputs.extend([z for z in rets if isinstance(z, Tensor)])\n",
    "        for t in outputs:\n",
    "            assert t.op is None, \"Error: should only have one op!!\"\n",
    "            t.op = self\n",
    "        # --\n",
    "        return rets\n",
    "\n",
    "    # forward the operation\n",
    "    def forward(self, *args, **kwargs):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    # backward with the pre-stored tensors\n",
    "    def backward(self):\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add6c1cd-03ec-421c-ac96-ce078ba941be",
   "metadata": {},
   "source": [
    "# Computational graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45da95a-d682-4879-8f6c-3d37f372cc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CG:\n",
    "    _cj: 'CG' = None\n",
    "    \n",
    "    def get_cg(cls, reset=False):\n",
    "        if CG._cj is None or reset:\n",
    "            CG._cj = CG()\n",
    "        return CG._cj\n",
    "\n",
    "    def __init__(self):\n",
    "        self.ops: List[Op] = []  # list of ops by execution order\n",
    "\n",
    "    def reg_op(self, op: Op):\n",
    "        assert op.idx is None\n",
    "        op.idx = len(self.ops)\n",
    "        self.ops.append(op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c9a4e825-c0f7-42fe-b71e-17bef32f2f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComputationGraph:\n",
    "    # global cg\n",
    "    _cg: 'ComputationGraph' = None\n",
    "\n",
    "    @classmethod\n",
    "    def get_cg(cls, reset=False):\n",
    "        if ComputationGraph._cg is None or reset:\n",
    "            ComputationGraph._cg = ComputationGraph()\n",
    "        return ComputationGraph._cg\n",
    "\n",
    "    def __init__(self):\n",
    "        self.ops: List[Op] = []  # list of ops by execution order\n",
    "\n",
    "    def reg_op(self, op: Op):\n",
    "        assert op.idx is None\n",
    "        op.idx = len(self.ops)\n",
    "        self.ops.append(op)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692362bc-c753-4524-b192-1c637d5fefb2",
   "metadata": {},
   "source": [
    "# Initializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c0089d27-2ad0-40f1-94bd-252ef0f0ceb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializer\n",
    "class Initializer:\n",
    "    @staticmethod\n",
    "    def uniform(shape: Sequence[int], a=0.0, b=0.2):\n",
    "        return xp.random.uniform(a, b, size=shape)\n",
    "\n",
    "    @staticmethod\n",
    "    def normal(shape: Sequence[int], mean=0., std=0.02):\n",
    "        return xp.random.normal(mean, std, size=shape)\n",
    "\n",
    "    @staticmethod\n",
    "    def constant(shape: Sequence[int], val=0.):\n",
    "        return xp.full(shape, val)\n",
    "\n",
    "    @staticmethod\n",
    "    def xavier_uniform(shape: Sequence[int], gain=1.0):\n",
    "        cap = math.sqrt(6) / sum(shape)\n",
    "        return Initializer.uniform(shape, a= -cap, b=cap)\n",
    "    \n",
    "    @staticmethod\n",
    "    def kaiming(shape, alpha=0):\n",
    "        std = math.sqrt(2/ (1+ math.pow(alpha, 2) * shape[0]))\n",
    "        return xp.random.normal(loc=0, scale=std, size=shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4fe7b0-187f-41c2-9f4d-fcbe99b298d8",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8438b3b9-e403-4a97-91c9-0ed59e68e30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model: collection of parameters\n",
    "class Model:\n",
    "    def __init__(self):\n",
    "        self.params: List[Parameter] = []\n",
    "\n",
    "    def add_parameters(self, shape, initializer='normal', **initializer_kwargs):\n",
    "        init_f = getattr(Initializer, initializer)\n",
    "        data = init_f(shape, **initializer_kwargs)\n",
    "        param = Parameter(data)\n",
    "        self.params.append(param)\n",
    "        return param\n",
    "\n",
    "    def save(self, path: str):\n",
    "        data = {f\"p{i}\": p.data for i,p in enumerate(self.params)}\n",
    "        xp.savez(path, **data)\n",
    "\n",
    "    def load(self, path: str):\n",
    "        data0 = xp.load(path)\n",
    "        data = {int(n[1:]):d for n,d in data0.items()}\n",
    "        for i,p in enumerate(self.params):\n",
    "            d = data[i]\n",
    "            assert d.shape == p.shape\n",
    "            p.data = d\n",
    "            \n",
    "            \n",
    "model = Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "05272361-6f3a-47f7-874c-56f42f7e5871",
   "metadata": {},
   "outputs": [],
   "source": [
    "W_emb = model.add_parameters((15000,100), initializer='kaiming')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "eddd9d58-d1eb-46e0-81ec-c3b3e906246a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.03962331, -1.17550892,  1.07855997, ..., -0.13250285,\n",
       "        -0.30320222, -1.02466026],\n",
       "       [ 2.71006767, -1.96917886,  0.35730203, ...,  0.63417175,\n",
       "         0.13013646,  1.13899194],\n",
       "       [-0.60605017, -1.35255062, -1.30008619, ...,  1.51134131,\n",
       "         2.4728064 , -1.3077591 ],\n",
       "       ...,\n",
       "       [ 1.59004232,  0.65861672, -1.7694316 , ...,  0.1707478 ,\n",
       "        -0.59836633,  0.91143301],\n",
       "       [-0.12156282,  0.48406735, -0.35170185, ...,  0.32102932,\n",
       "         1.02227212,  0.43405295],\n",
       "       [ 0.67314798,  1.79505314,  0.59410205, ..., -0.10629687,\n",
       "        -0.94908697,  2.53709584]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_emb.data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be9154a-a7ba-41e2-a46e-9dfe8683f085",
   "metadata": {},
   "source": [
    "# Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b9a6b5cf-c59c-4495-8948-1f4b88bb0f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer\n",
    "class Trainer:\n",
    "    def __init__(self, model: Model):\n",
    "        self.model = model\n",
    "\n",
    "    def clone_param_stats(self, model: Model):\n",
    "        clone = list()\n",
    "        for param in model.params:\n",
    "            clone.append(np.zeros(param.data.shape))\n",
    "        return clone\n",
    "\n",
    "    def update(self):  # to be implemented\n",
    "        raise NotImplementedError()\n",
    "\n",
    "class SGDTrainer(Trainer):\n",
    "    def __init__(self, model: Model, lrate=0.1):\n",
    "        super().__init__(model)\n",
    "        self.lrate = lrate\n",
    "\n",
    "    def update(self):\n",
    "        lrate = self.lrate\n",
    "        for p in self.model.params:\n",
    "            if p.grad is not None:\n",
    "                if isinstance(p.grad, dict):  # sparsely update to save time!\n",
    "                    self.update_sparse(p, p.grad, lrate)\n",
    "                else:\n",
    "                    self.update_dense(p, p.grad, lrate)\n",
    "            # clean grad\n",
    "            p.grad = None\n",
    "        # --\n",
    "\n",
    "    def update_dense(self, p: Parameter, g: xp.ndarray, lrate: float):\n",
    "        p.data -= lrate * g\n",
    "\n",
    "    def update_sparse(self, p: Parameter, gs: Dict[int, xp.ndarray], lrate: float):\n",
    "        for widx, arr in gs.items():\n",
    "            p.data[widx] -= lrate * arr\n",
    "\n",
    "\n",
    "class MomentumTrainer(Trainer):\n",
    "    def __init__(self, model: Model, lrate=0.1, mrate=0.99):\n",
    "        raise NotImplementedError\n",
    "\n",
    "# --"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa57faa-8a15-415e-b786-4ab67514ccf5",
   "metadata": {},
   "source": [
    "# Graph computation algorithms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5dbe21a-7f8e-40a4-b311-643906d0205e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def reset_computation_graph():\n",
    "    return ComputationGraph.get_cg(reset=True)\n",
    "\n",
    "def forward(t: Tensor):\n",
    "    # since we calculate greedily, the result are already there!!\n",
    "    return asnumpy(t.data)\n",
    "\n",
    "def backward(t: Tensor, alpha=1.):\n",
    "    # first put grad to the start one\n",
    "    t.accumulate_grad(alpha)\n",
    "    # locate the op\n",
    "    op = t.op\n",
    "    assert op is not None, \"Cannot backward on tensor since no op!!\"\n",
    "    # backward the whole graph!!\n",
    "    cg = ComputationGraph.get_cg()\n",
    "    for idx in reversed(range(op.idx+1)):\n",
    "        cg.ops[idx].backward()\n",
    "    # --\n",
    "\n",
    "### Helper\n",
    "def log_softmax(x: xp.ndarray, axis=-1):\n",
    "    c = xp.max(x, axis=axis, keepdims=True)  # [*, 1, *]\n",
    "    x2 = x - c  # [*, ?, *]\n",
    "    logsumexp = xp.log(xp.exp(x2).sum(axis=axis, keepdims=True))  # [*, 1, *]\n",
    "    return x2 - logsumexp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35183eb-4065-4cd0-8a24-bfdf42cbba9d",
   "metadata": {},
   "source": [
    "## Backpropable functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b0ffd50-3f9e-4322-a1cb-6409550fc3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OpLookup(Op):\n",
    "    def __init__(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "class OpSum(Op):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    # [..., K, ...] -> [..., ...]\n",
    "    def forward(self, emb: Tensor, axis: int):\n",
    "        reduce_size = emb.data.shape[axis]\n",
    "        arr_sum = emb.data.sum(axis=axis)\n",
    "        t_sum = Tensor(arr_sum)\n",
    "        self.store_ctx(emb=emb, t_sum=t_sum, axis=axis, reduce_size=reduce_size)\n",
    "        return t_sum\n",
    "\n",
    "    def backward(self):\n",
    "        emb, t_sum, axis, reduce_size = self.get_ctx('emb', 't_sum', 'axis', 'reduce_size')\n",
    "        if t_sum.grad is not None:\n",
    "            g0 = xp.expand_dims(t_sum.grad, axis)\n",
    "            g = xp.repeat(g0, reduce_size, axis=axis)\n",
    "            emb.accumulate_grad(g)\n",
    "        # --\n",
    "\n",
    "class OpDot(Op):\n",
    "    def __init__(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "class OpTanh(Op):\n",
    "    def __init__(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "class OpRelu(Op):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    # [N] -> [N]\n",
    "    def forward(self, t: Tensor):\n",
    "        arr_relu = t.data  # [N]\n",
    "        arr_relu[arr_relu < 0.0] = 0.0\n",
    "        t_relu = Tensor(arr_relu)\n",
    "        self.store_ctx(t=t, t_relu=t_relu, arr_relu=arr_relu)\n",
    "        return t_relu\n",
    "\n",
    "    def backward(self):\n",
    "        t, t_relu, arr_relu = self.get_ctx('t', 't_relu', 'arr_relu')\n",
    "        if t_relu.grad is not None:\n",
    "            grad_t = xp.where(arr_relu > 0.0, 1.0, 0.0) * t_relu.grad  # [N]\n",
    "            t.accumulate_grad(grad_t)\n",
    "        # --\n",
    "class OpLogloss(Op):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    # [*, N], [*] -> [*]\n",
    "    def forward(self, logits: Tensor, tags: Union[int, List[int]]):\n",
    "        # negative log likelihood\n",
    "        arr_tags = xp.asarray(tags)  # [*]\n",
    "        arr_logprobs = log_softmax(logits.data)  # [*, N]\n",
    "        if len(arr_logprobs.shape) == 1:\n",
    "            arr_nll = - arr_logprobs[arr_tags]  # []\n",
    "        else:\n",
    "            assert len(arr_logprobs.shape) == 2\n",
    "            arr_nll = - arr_logprobs[xp.arange(len(arr_logprobs.shape[0])), arr_tags]  # [*]\n",
    "        loss_t = Tensor(arr_nll)\n",
    "        self.store_ctx(logits=logits, loss_t=loss_t, arr_tags=arr_tags, arr_logprobs=arr_logprobs)\n",
    "        return loss_t\n",
    "\n",
    "    def backward(self):\n",
    "        logits, loss_t, arr_tags, arr_logprobs = self.get_ctx('logits', 'loss_t', 'arr_tags', 'arr_logprobs')\n",
    "        if loss_t.grad is not None:\n",
    "            arr_probs = xp.exp(arr_logprobs)  # [*, N]\n",
    "            grad_logits = arr_probs  # prob-1 for gold, prob for non-gold\n",
    "            if len(grad_logits.shape) == 1:\n",
    "                grad_logits[arr_tags] -= 1.\n",
    "                grad_logits *= loss_t.grad\n",
    "            else:\n",
    "                grad_logits[xp.arange(len(grad_logits.shape[0])), arr_tags] -= 1.\n",
    "                grad_logits *= loss_t.grad[:,None]\n",
    "            logits.accumulate_grad(grad_logits)\n",
    "        # --\n",
    "        \n",
    "class OpAdd(Op):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, a: Tensor, b: Tensor, alpha_a=1., alpha_b=1.):\n",
    "        if b is None:\n",
    "            arr_add = alpha_a * a.data\n",
    "        else:\n",
    "            arr_add = alpha_a * a.data + alpha_b * b.data\n",
    "        t_add = Tensor(arr_add)\n",
    "        self.store_ctx(a=a, b=b, t_add=t_add, alpha_a=alpha_a, alpha_b=alpha_b)\n",
    "        return t_add\n",
    "\n",
    "    def backward(self):\n",
    "        a, b, t_add, alpha_a, alpha_b = self.get_ctx('a', 'b', 't_add', 'alpha_a', 'alpha_b')\n",
    "        if t_add.grad is not None:\n",
    "            a.accumulate_grad(alpha_a * t_add.grad)\n",
    "            if b is not None:\n",
    "                b.accumulate_grad(alpha_b * t_add.grad)\n",
    "        # --\n",
    "class OpDropout(Op):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x: Tensor, drop: float, is_training: bool):\n",
    "        if is_training:\n",
    "            arr_mask = xp.random.binomial(1, 1.-drop, x.shape) * (1./(1-drop))\n",
    "            arr_drop = (x.data * arr_mask)\n",
    "            t_drop = Tensor(arr_drop)\n",
    "        else:\n",
    "            arr_mask = 1.\n",
    "            t_drop = Tensor(x.data)  # note: here copy things to make it consistent!\n",
    "        self.store_ctx(is_training=is_training, x=x, arr_mask=arr_mask, t_drop=t_drop)\n",
    "        return t_drop\n",
    "\n",
    "    def backward(self):\n",
    "        is_training, x, arr_mask, t_drop = self.get_ctx('is_training', 'x', 'arr_mask', 't_drop')\n",
    "        if not is_training:\n",
    "            pass\n",
    "            # print(\"Warn: Should not backward if not in training??\")\n",
    "        if t_drop.grad is not None:\n",
    "            x.accumulate_grad(arr_mask * t_drop.grad)\n",
    "        # --"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387cb4ea-5700-4c2f-808a-94e2bd90f7ff",
   "metadata": {},
   "source": [
    "## Shortcuts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c6fe1b6-e97c-447b-b66e-b09995f74c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --\n",
    "# \n",
    "def lookup(W_emb, words): return OpLookup().full_forward(W_emb, words)\n",
    "def sum(emb, axis): return OpSum().full_forward(emb, axis)\n",
    "def dot(W_h_i, h): return OpDot().full_forward(W_h_i, h)\n",
    "def tanh(param): return OpTanh().full_forward(param)\n",
    "def relu(param): return OpRelu().full_forward(param)\n",
    "def log_loss(my_scores, tag): return OpLogloss().full_forward(my_scores, tag)\n",
    "def dropout(x, drop, is_training): return OpDropout().full_forward(x, drop, is_training)\n",
    "# --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cdb521a9-79dd-4dd8-a438-6488105cef13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a53e829-e317-4e0d-a9b0-31c80bea2734",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "822ba19b-f072-43b3-8d68-a56644b13869",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08dd210b-88b8-41bc-9c59-d8ed8d748b6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
